{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STQP23IX6ETU"
   },
   "source": [
    "**Conversational memory** is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
    "\n",
    "The memory allows a \"agent\" to remember previous interactions with the user. By default, agents are stateless — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
    "\n",
    "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
    "\n",
    "https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_btuoIl51ep",
    "outputId": "982a2931-85d2-4683-b54e-f678230adba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/425.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/425.6 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.6/425.6 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "NslAcGKC6UmK"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from getpass import getpass\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, ConversationSummaryMemory, ConversationBufferWindowMemory, ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbVV58et6iG6"
   },
   "source": [
    "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted.\n",
    "https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otYKuHk-6jUl",
    "outputId": "eeeae1dc-8462-4898-eebf-2f6c2b5cde44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "igmS2tTv7Ik6"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    temperature=0, \n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='text-davinci-003'  # can be used with llms like 'gpt-3.5-turbo'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glmgrof-7dM0"
   },
   "source": [
    "Later we will make use of a count_tokens utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uwIKryZq7d_U"
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "sn0p6oDX7jxM"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RU1r8P_A7x74",
    "outputId": "e6af4575-c97b-4946-fcf2-08fee5edfdb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpsN_UWR7-gj"
   },
   "source": [
    "Interesting! So this chain's prompt is telling it to chat with the user and try to give truthful answers. If we look closely, there is a new component in the prompt that we didn't see when we were tinkering with the LLMMathChain: history. This is where our memory will come into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feCACQta7_Ov",
    "outputId": "9b472419-de95-4b71-edd2-8dd56aeb5863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WdIjw6_8F9R"
   },
   "source": [
    "Nothing really magical going on here, just a straightforward pass through an LLM. In fact, this chain inherits these methods directly from the LLMChain without any modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JX8sG1t8Gg3",
    "outputId": "0e5dc73e-df47-4046-ecf7-788eb6c73344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
      "        return self.apply([inputs])[0]\n",
      "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
      "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
      "        response = self.generate(input_list)\n",
      "        return self.create_outputs(response)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brnRQMnG8JRt"
   },
   "source": [
    "So basically this chain combines an input from the user with the conversation history to generate a meaningful (and hopefully truthful) response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nucsGlXm8Thv"
   },
   "source": [
    "**Memory type #1: ConversationBufferMemory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDRyXtun8W4h"
   },
   "source": [
    "The ConversationBufferMemory does just what its name suggests: it keeps a buffer of the previous conversation excerpts as part of the context in the prompt.\n",
    "\n",
    "Key feature: the conversation buffer memory keeps the previous pieces of conversation completely unmodified, in their raw form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "OZmywgnl8bE0"
   },
   "outputs": [],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A13nHjXs8Uux"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOjTdu8t8jBB"
   },
   "source": [
    "We pass a user prompt the the ConversationBufferMemory like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gx-b6xH8j_x",
    "outputId": "7b643884-11e4-4950-8a57-3032c14a3ec4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': \" Good morning! It's a beautiful day today, isn't it? How can I help you?\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8ehfCih8ucP"
   },
   "source": [
    "This one call used a total of 85 tokens, but we can't see that from the above. If we'd like to count the number of tokens being used we just pass our conversation chain object and the message we'd like to input via the count_tokens function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "SXVF54oQ8vGl",
    "outputId": "9b501e02-a1ea-498b-ae5c-01657dca7d39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 162 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Ah, recursion! That's a fascinating concept. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve. Do you have any specific questions about recursion?\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "     \"My interest here is to learn about recursion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "IPIkW3iQ80j8",
    "outputId": "0253715b-332a-4f98-f537-8f3b1d920a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 263 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's an example of a recursive function written in Python:\\n\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\nThis function calculates the factorial of a given number. It calls itself until the base case (n == 0) is reached, at which point it returns the result.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf,\n",
    "    \"can you show me an example with python code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "8a2Aviad83rO",
    "outputId": "1bbb5b77-c6ff-4295-97cb-e8f9f48e3328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 391 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's another example of a recursive function written in Python:\\n\\ndef fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nThis function calculates the nth number in the Fibonacci sequence. It calls itself twice, once for the n-1th number and once for the n-2th number, and then adds the two results together.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"show me more example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "EJM3qbvp9COE",
    "outputId": "201355c8-9553-43e2-bf78-7c5569b1a486"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 455 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Your aim is to learn about recursion. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_buf, \n",
    "    \"What is my aim again? \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPkzoZUP9Hr9"
   },
   "source": [
    "Our LLM with ConversationBufferMemory can clearly remember earlier interactions in the conversation. Let's take a closer look to how the LLM is saving our previous conversation. We can do this by accessing the .buffer attribute for the .memory in our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPR8b-dr9IZA",
    "outputId": "b1b12260-81dc-4e54-93db-1002303abdfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to learn about recursion\n",
      "AI:  Ah, recursion! That's a fascinating concept. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve. Do you have any specific questions about recursion?\n",
      "Human: can you show me an example with python code\n",
      "AI:  Sure! Here's an example of a recursive function written in Python:\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "This function calculates the factorial of a given number. It calls itself until the base case (n == 0) is reached, at which point it returns the result.\n",
      "Human: show me more example\n",
      "AI:  Sure! Here's another example of a recursive function written in Python:\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "This function calculates the nth number in the Fibonacci sequence. It calls itself twice, once for the n-1th number and once for the n-2th number, and then adds the two results together.\n",
      "Human: What is my aim again? \n",
      "AI:  Your aim is to learn about recursion. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oc5YDq9QTz"
   },
   "source": [
    "**Memory type #2: ConversationSummaryMemory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSpBu3pi9R0B"
   },
   "source": [
    "The problem with the ConversationBufferMemory is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed.\n",
    "\n",
    "Enter ConversationSummaryMemory.\n",
    "\n",
    "Again, we can infer from the name what is going on.. we will keep a summary of our previous conversation snippets as our history. How will we summarize these? LLM to the rescue.\n",
    "\n",
    "Key feature: the conversation summary memory keeps the previous pieces of conversation in a summarized form, where the summarization is performed by an LLM.\n",
    "\n",
    "In this case we need to send the llm to our memory constructor to power its summarization ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "tuICJ57C9Ufa"
   },
   "outputs": [],
   "source": [
    "conversation_sum = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL4uEooB9eUQ"
   },
   "source": [
    "When we have an llm, we always have a prompt ;) Let's see what's going on inside our conversation summary memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuiLe9i49e4H",
    "outputId": "c8422e62-772e-4bdd-8d43-21f7f9fdf760"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
      "\n",
      "EXAMPLE\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: Why do you think artificial intelligence is a force for good?\n",
      "AI: Because artificial intelligence will help humans reach their full potential.\n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "END OF EXAMPLE\n",
      "\n",
      "Current summary:\n",
      "{summary}\n",
      "\n",
      "New lines of conversation:\n",
      "{new_lines}\n",
      "\n",
      "New summary:\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU73lN6d9lVH"
   },
   "source": [
    "Cool! So each new interaction is summarized and appended to a running summary as the memory of our chain. Let's see how this works in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "N-K0MXHR9mAV",
    "outputId": "704280b8-248a-4af2-c8e8-41f0f1bcc7a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 283 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
    "# but let's keep track of our tokens:\n",
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "z0Exwlg19ohF",
    "outputId": "eb7bcde3-b69b-48e0-f28e-336a0d62681f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 484 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hi there! I'm glad you're interested in learning about recursion. Recursion is a programming technique that involves a function calling itself repeatedly until a certain condition is met. It's a powerful tool for solving complex problems, but it can be difficult to understand and implement. Can I answer any specific questions you have about recursion?\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"My interest here is to learn about recursion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rleX2JzP9ukQ",
    "outputId": "70ff8a8a-939b-46cc-dcdd-3e6584b2598d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 702 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Sure! Here is an example of a recursive function written in Python:\\n\\ndef recursive_function(n):\\n    if n == 0:\\n        return 0\\n    else:\\n        return n + recursive_function(n-1)\\n\\nThis function takes an integer as an argument and returns the sum of all the integers from 0 to the argument. For example, if you call recursive_function(5), it will return 15 (0 + 1 + 2 + 3 + 4 + 5).'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"can you show me an example with python code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "NIvy0D8r9zqF",
    "outputId": "84173c19-7d89-4f29-df87-91477accbdee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 759 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's another example of a recursive function written in Python. This one takes a string as an argument and returns the reverse of the string.\\n\\ndef reverse_string(string):\\n    if len(string) == 0:\\n        return string\\n    else:\\n        return reverse_string(string[1:]) + string[0]\\n\\nTry it out by passing in a string of your choice!\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"show me more example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "rqeHzJyO94Um",
    "outputId": "03080b45-b84e-4609-f238-84579e24f7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 747 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Your aim is to learn about recursion and understand how to implement it. I can provide more examples of recursive functions written in Python if you'd like.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum, \n",
    "    \"What is my aim again?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rt-LjTvs97Cq",
    "outputId": "6c3988bb-baa4-45ac-a648-e07f882a8587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The human greeted the AI and the AI responded with a greeting and asked how it could help. The human expressed interest in learning about recursion, to which the AI responded by explaining that recursion is a powerful tool for solving complex problems, but can be difficult to understand and implement. The AI then asked if it could answer any specific questions the human had about recursion, and provided an example of a recursive function written in Python which takes an integer as an argument and returns the sum of all the integers from 0 to the argument. When the human asked for more examples, the AI provided another example of a recursive function written in Python which takes a string as an argument and returns the reverse of the string, and reminded the human that their aim was to learn about recursion and understand how to implement it, offering to provide more examples if needed.\n"
     ]
    }
   ],
   "source": [
    "print(conversation_sum.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tyMEv9V-AX6"
   },
   "source": [
    "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
    "\n",
    "We can count the number of tokens being used (without making a call to OpenAI) using the tiktoken tokenizer like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptRH67Wc-BLb",
    "outputId": "c57de462-0073-40e5-d12d-c1bd26fdebf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 401\n",
      "Summary memory conversation length: 171\n"
     ]
    }
   ],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = tiktoken.encoding_for_model('text-davinci-003')\n",
    "\n",
    "# show number of tokens for the memory used by each memory type\n",
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6_9RNx0-IfI"
   },
   "source": [
    " **Note: the text-davinci-003 and gpt-3.5-turbo models have a large max tokens count of 4096 tokens between prompt and answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAyRpLam-LCc"
   },
   "source": [
    "**Memory type #3: ConversationBufferWindowMemory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbjDMYpV-RLX"
   },
   "source": [
    "Another great option for these cases is the ConversationBufferWindowMemory where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count and the per-call token count will drop noticeably. We will control this window with the k parameter.\n",
    "\n",
    "Key feature: the conversation buffer window memory keeps the latest pieces of the conversation in raw form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "umGYBHJC-Tnj"
   },
   "outputs": [],
   "source": [
    "conversation_bufw = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationBufferWindowMemory(k=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "7nhh1KHy-Wfp",
    "outputId": "77ea0b51-c287-4b56-cea8-229e2df251ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 85 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "4mhxzkOm-cS4",
    "outputId": "4106c8d5-aae3-4a6b-caa1-88a331ed6282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 162 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Ah, recursion! That's a fascinating concept. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve. Do you have any specific questions about recursion?\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"My interest here is to learn about recursion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "6XyuFvVo-g8M",
    "outputId": "88cc2f05-966b-4fb9-c3ae-c07b4f9eaa0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 232 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's an example of a recursive function written in Python:\\n\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\nThis function calculates the factorial of a given number. It calls itself until the base case (n == 0) is reached, at which point it returns the result.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"can you show me an example with python code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "JJj1VGPF-ixu",
    "outputId": "93d8b0ae-de76-40f7-c1f8-eadf44b72b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 206 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's an example of a for loop written in Python:\\n\\nfor i in range(10):\\n    print(i)\\n\\nThis loop prints out the numbers 0 through 9.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"show me more example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DEPO41vr-nnL",
    "outputId": "cb4f3a77-2caf-4929-bbf9-8e2bd7df247c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 131 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Your aim is to understand how for loops work in Python.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_bufw, \n",
    "    \"What is my aim again? \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qvz_KL8V-uSC"
   },
   "source": [
    "As we can see, it effectively 'forgot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be 1, we would expect it remembers only the last interaction.\n",
    "\n",
    "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "JaKqQ49D-vzS"
   },
   "outputs": [],
   "source": [
    "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OwAMhdc-y8N",
    "outputId": "cb5bdf02-8c21-497f-ebe2-75ecade4f0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my aim again? \n",
      "AI:  Your aim is to understand how for loops work in Python.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT-d___Q-3tb"
   },
   "source": [
    "Makes sense.\n",
    "\n",
    "On the plus side, we are shortening our conversation length when compared to buffer memory without a window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGkDwWO8-4Pr",
    "outputId": "033a7fdc-2a1a-43ce-8582-f681797ed437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer memory conversation length: 401\n",
      "Summary memory conversation length: 171\n",
      "Buffer window memory conversation length: 25\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
    "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
    "    f'Buffer window memory conversation length: {len(tokenizer.encode(bufw_history))}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdtz_kpW--fo"
   },
   "source": [
    "*Note: We are using k=2 here for illustrative purposes, in most real world applications you would need a higher value for k.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbA-6Gk4YYBa"
   },
   "source": [
    "**Memory type #4 ConversationSummaryBufferMemory**\n",
    "\n",
    "\n",
    "Key feature: the conversation summary memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "3nShAEQ2YlP8"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "FCp7EZwsbMVD"
   },
   "outputs": [],
   "source": [
    "conversation_sum_bufw = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryBufferMemory(llm=llm,max_token_limit=650) #same like previous k=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "gXXkzKMhbgyi",
    "outputId": "a5769a6f-3e52-4a6e-c6e1-dd69bb8267af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 85 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Good morning! It's a beautiful day today, isn't it? How can I help you?\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum_bufw, \n",
    "    \"Good morning AI!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "5qwsLGFVeNiJ",
    "outputId": "f31fd1a1-236e-48c1-8ff4-4e2b551ff6cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 162 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Ah, recursion! That's a fascinating concept. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve. Do you have any specific questions about recursion?\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum_bufw, \n",
    "    \"My interest here is to learn about recursion\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "YloEdYWYeVRL",
    "outputId": "46f08723-550e-48df-ddf1-70f7942bd96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 263 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's an example of a recursive function written in Python:\\n\\ndef factorial(n):\\n    if n == 0:\\n        return 1\\n    else:\\n        return n * factorial(n-1)\\n\\nThis function calculates the factorial of a given number. It calls itself until the base case (n == 0) is reached, at which point it returns the result.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum_bufw, \n",
    "    \"can you show me an example with python code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "rLVL5vytec0Q",
    "outputId": "9af6960f-41fa-4f92-f40f-6ee671955355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 391 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Sure! Here's another example of a recursive function written in Python:\\n\\ndef fibonacci(n):\\n    if n == 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\nThis function calculates the nth number in the Fibonacci sequence. It calls itself twice, once for the n-1th number and once for the n-2th number, and then adds the two results together.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum_bufw, \n",
    "   \"show me more example\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "WxKOX4USejP4",
    "outputId": "e98b2dd4-df5d-4057-9464-723b078220ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 455 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Your aim is to learn about recursion. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_sum_bufw, \n",
    "    \"What is my aim again? \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "tweNCvsRer-Q"
   },
   "outputs": [],
   "source": [
    "bufw_history = conversation_sum_bufw.memory.load_memory_variables(\n",
    "    inputs=[]\n",
    ")['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILi8jNcMe1mV",
    "outputId": "d4c36ea6-dc3f-4737-b121-a89609e98f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?\n",
      "Human: My interest here is to learn about recursion\n",
      "AI:  Ah, recursion! That's a fascinating concept. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve. Do you have any specific questions about recursion?\n",
      "Human: can you show me an example with python code\n",
      "AI:  Sure! Here's an example of a recursive function written in Python:\n",
      "\n",
      "def factorial(n):\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "This function calculates the factorial of a given number. It calls itself until the base case (n == 0) is reached, at which point it returns the result.\n",
      "Human: show me more example\n",
      "AI:  Sure! Here's another example of a recursive function written in Python:\n",
      "\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "This function calculates the nth number in the Fibonacci sequence. It calls itself twice, once for the n-1th number and once for the n-2th number, and then adds the two results together.\n",
      "Human: What is my aim again? \n",
      "AI:  Your aim is to learn about recursion. Recursion is a programming technique where a function calls itself, allowing it to repeat a task until a certain condition is met. It's often used to solve complex problems that would otherwise be difficult to solve.\n"
     ]
    }
   ],
   "source": [
    "print(bufw_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQpJ1kxX_rOq"
   },
   "source": [
    "**Memory type #5 :ConversationKnowledgeGraphMemory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhEluO9I_1K5"
   },
   "source": [
    "This is a super cool memory type that was introduced just recently. It is based on the concept of a knowledge graph which recognizes different entities and connects them in pairs with a predicate resulting in (subject, predicate, object) triplets. This enables us to compress a lot of information into highly significant snippets that can be fed into the model as context. \n",
    "\n",
    "Key feature: the conversation knowledge graph memory keeps a knowledge graph of all the entities that have been mentioned in the interactions together with their semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "-cO1inTN_6pd"
   },
   "outputs": [],
   "source": [
    "!pip install -qU networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "bUB-p-nsAAMP"
   },
   "outputs": [],
   "source": [
    "conversation_kg = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "OQrCK3rNACvP",
    "outputId": "ca5a16de-c275-49a5-fd77-855bd9a1cbbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 1167 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" Hi Human! My name is AI. It's nice to meet you. I like mangoes too! Did you know that mangoes are a great source of vitamins A and C?\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(\n",
    "    conversation_kg, \n",
    "    \"My name is human and I like mangoes!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uP4FSVWuAJKk"
   },
   "source": [
    "the memory keeps a knowledge graph of everything it learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jq0nxF3RAMAB",
    "outputId": "51add483-39ea-4912-f5f4-f5ea547cd10c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Human', 'human', 'name'), ('Human', 'mangoes', 'likes')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_kg.memory.kg.get_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-10qgCqaAKNn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
